MONGODB_URL=mongodb://localhost:27017
MONGODB_DB_NAME=chat-ui
MONGODB_DIRECT_CONNECTION=false

COOKIE_NAME=hf-chat
HF_TOKEN=#hf_<token> from https://huggingface.co/settings/token
HF_API_ROOT=https://api-inference.huggingface.co/models

PUBLIC_APP_NAME=ChatUI for AWS
PUBLIC_APP_ASSETS=huggingchat
PUBLIC_APP_COLOR=blue
PUBLIC_APP_DATA_SHARING=1
PUBLIC_APP_DESCRIPTION="Demo LLMs on AWS Inferentia 2 with ChatUI."
PUBLIC_APP_DISCLAIMER=1
PUBLIC_APP_DISCLAIMER_MESSAGE="Disclaimer: AI is an area of active research with known problems such as biased generation and misinformation. Do not use this application for high-stakes decisions or advice."

PUBLIC_ORIGIN=
PUBLIC_SHARE_PREFIX=

ENABLE_ASSISTANTS=false
ENABLE_ASSISTANTS_RAG=false
EXPOSE_API=true


APP_BASE=""


PUBLIC_ANNOUNCEMENT_BANNERS=`[
]`
LLM_SUMMERIZATION=true

MODELS=`[
    {
      "name": "NousResearch/Meta-Llama-3-8B-Instruct",
      "description": "Generation over generation, Meta Llama 3 demonstrates state-of-the-art performance on a wide range of industry benchmarks and offers new capabilities, including improved reasoning.",
      "logoUrl": "https://huggingface.co/datasets/huggingchat/models-logo/resolve/main/meta-logo.png",
      "modelUrl": "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
      "websiteUrl": "https://llama.meta.com/llama3/",
      "preprompt": "This is a conversation between User and Llama, a friendly chatbot. Llama is helpful, kind, honest, good at writing, and never fails to answer any requests immediately and with precision.",
      "chatPromptTemplate": "<|begin_of_text|>{{#if @root.preprompt}}<|start_header_id|>system<|end_header_id|>\n\n{{@root.preprompt}}<|eot_id|>{{/if}}{{#each messages}}<|start_header_id|>user<|end_header_id|>\n\n{{content}}<|eot_id|><|start_header_id|>assistant<|end_header_id|>{{/each}}",
      "parameters": {
        "stop": ["<|eot_id|>", "<|end_of_text|>"],
        "truncate": 256,
        "max_new_tokens": 768
      },
      "promptExamples" : [
      {
        "title": "Write an email from bullet list",
        "prompt": "As a restaurant owner, write a professional email to the supplier to get these products every week: \n\n- Wine (x10)\n- Eggs (x24)\n- Bread (x12)"
      }, {
        "title": "Code a snake game",
        "prompt": "Code a basic snake game in python, give explanations for each step."
      }, {
        "title": "Assist in a task",
        "prompt": "How do I make a delicious lemon cheesecake?"
      }
      ],
      "endpoints": [
        {
         "url": "http://127.0.0.1:8080",
         "type": "tgi"
        }
       ],
    },
    {
      "name": "mistralai/Mistral-7B-Instruct-v0.2",
      "displayName": "mistralai/Mistral-7B-Instruct-v0.2",
      "description": "Mistral 7B is a new Apache 2.0 model, released by Mistral AI that outperforms Llama2 13B in benchmarks.",
      "websiteUrl": "https://mistral.ai/news/announcing-mistral-7b/",
      "preprompt": "",
      "chatPromptTemplate" : "<s>{{#each messages}}{{#ifUser}}[INST] {{#if @first}}{{#if @root.preprompt}}{{@root.preprompt}}\n{{/if}}{{/if}}{{content}} [/INST]{{/ifUser}}{{#ifAssistant}}{{content}}</s>{{/ifAssistant}}{{/each}}",
      "parameters": {
        "temperature": 0.1,
        "top_p": 0.95,
        "repetition_penalty": 1.2,
        "top_k": 50,
        "max_new_tokens": 768,
        "stop": ["</s>"]
      },
      "endpoints": [
        {
         "url": "http://127.0.0.1:8080",
         "type": "tgi"
        }
      ],
      "promptExamples": [
        {
          "title": "Write an email from bullet list",
          "prompt": "As a restaurant owner, write a professional email to the supplier to get these products every week: \n\n- Wine (x10)\n- Eggs (x24)\n- Bread (x12)"
        }, {
          "title": "Code a snake game",
          "prompt": "Code a basic snake game in python, give explanations for each step."
        }, {
          "title": "Assist in a task",
          "prompt": "How do I make a delicious lemon cheesecake?"
        }
      ]
    },
    {
      "name": "codellama/CodeLlama-7b-Instruct-hf",
      "displayName": "codellama/CodeLlama-7b-Instruct-hf",
      "description": "Code Llama, a state of the art code model from Meta. Now in 7B!",
      "logoUrl": "https://huggingface.co/datasets/huggingchat/models-logo/resolve/main/meta-logo.png",
      "websiteUrl": "https://ai.meta.com/blog/code-llama-large-language-model-coding/",
      "modelUrl": "https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf",
      "preprompt": "",
      "chatPromptTemplate" : "<s>[INST] <<SYS>>\n{{preprompt}}\n<</SYS>>\n\n{{#each messages}}{{#ifUser}}{{content}} [/INST] {{/ifUser}}{{/each}}",
       "endpoints": [
        {
         "url": "http://127.0.0.1:8080",
         "type": "tgi"
        }
      ],
      "promptExamples": [
        {
          "title": "Fibonacci in Python",
          "prompt": "Write a python function to calculate the nth fibonacci number."
        }, {
          "title": "JavaScript promises",
          "prompt": "How can I wait for multiple JavaScript promises to fulfill before doing something with their values?"
        }, {
          "title": "Rust filesystem",
          "prompt": "How can I load a file from disk in Rust?"
        }
      ],
      "parameters": {
        "temperature": 0.1,
        "top_p": 0.95,
        "repetition_penalty": 1.2,
        "top_k": 50,
        "truncate": 256,
        "max_new_tokens": 768,
        "stop": ["</s>", "<step>", " <step>", " <step> "],
      }
    },
]`
